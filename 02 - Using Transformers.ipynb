{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44927bc-9ba0-4fdb-9549-32939bc7cb30",
   "metadata": {},
   "source": [
    "# Behind the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ea92f-2d35-4cc9-8668-cff460a6018c",
   "metadata": {},
   "source": [
    "## Processing with a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574a498-aa42-46c0-aba9-8ab0ba7ccf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a435b-779a-4e57-b009-10260a8bb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa4fb3-145e-4d18-baca-7a033106d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much.\"\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24adad0-6758-4f57-8bd7-3fb9b80d6d58",
   "metadata": {},
   "source": [
    "## Going through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc414278-aa9b-4fd4-9f3f-3be84d888a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb567b0-1b2a-441f-ab8f-72a7f1883ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fd7d4-dd69-4165-878b-687e633783d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9873fa-eaee-443f-b024-f46578410044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94c711-1b79-41fd-afec-33caa2042b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79926b89-a343-45d4-b942-32c1f48a7915",
   "metadata": {},
   "source": [
    "## Postprocessing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de990101-3446-45f1-9159-eab8561cbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94320fe1-d10c-46b2-abd5-be5125d146e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeccdb6-64e5-4d99-9f0f-7a315487ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c55c2-747a-42f4-8d43-3a2ea8302405",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c39dfe-328c-4ea1-8901-690e938a196b",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849cf25-9e26-4983-af73-fe869020aac8",
   "metadata": {},
   "source": [
    "## Creating a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc83d4-57ab-4cd5-bd68-d02ba675cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed1fbb-1dd0-4121-a66d-7f23e5c714f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b7057-0097-4962-991b-8371db65c36c",
   "metadata": {},
   "source": [
    "## Loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cd60b-96db-4123-9e68-a053da7e3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0205e8-4358-4488-b6f1-5886ca69394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = AutoModel.from_pretrained(\"models/bert-base-cased/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a257c5-cc46-4a33-b7c9-6d55f0430365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048c53b-dae7-4d2b-9dd7-13125d44d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"my-awesome-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc8a6e-fdc3-4675-8711-fe0003d89f7a",
   "metadata": {},
   "source": [
    "## Encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a569fe-c99a-44bb-90bd-484596a462d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec7d9e-2801-4568-941e-f3a684cd38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13e33c-204b-4619-baed-b198429eb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9066e49-4b7a-457f-a10e-dba4c65a374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28b791-a330-4495-92cd-4eaadbf7a912",
   "metadata": {},
   "source": [
    "### Padding inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ed510-c773-40a8-a171-c57e0af64460",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\n",
    "    [\"How are you?\", \"I'm fine, thank you!\"], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a3f17-5713-4e0d-898b-0cd0e7af5627",
   "metadata": {},
   "source": [
    "### Truncating inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2acb36-4ae9-4709-80c4-0dab7e585dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\n",
    "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
    "    truncation=True,\n",
    ")\n",
    "print(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7e9e7-b794-4a37-b0bf-76ac65be4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\n",
    "    [\"How are you?\", \"I'm fine, thank you!\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=5,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbb3ac-a083-4028-82bd-799236acbec5",
   "metadata": {},
   "source": [
    "### Adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808c82e-cc38-4e7e-833a-0afe4bd4efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"How are you?\")\n",
    "print(encoded_input[\"input_ids\"])\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50355b3a-7acb-48ba-9de2-4b3029c002ec",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0762e-7d34-465e-867e-4e6b6a96f0f4",
   "metadata": {},
   "source": [
    "## Word-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220142b0-87a5-4c76-ac5e-2a1b957066d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e647c54-0b59-49df-b09f-0b08650c6986",
   "metadata": {},
   "source": [
    "## Character-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a6d6c-9c6c-46d6-b0d1-baaaf4f76dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(\"Jim Henson was a puppeteer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972a832-570d-4cc3-b975-c08e30e83d36",
   "metadata": {},
   "source": [
    "## Subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5bdbb-8542-4b2b-af46-c455faf93eb1",
   "metadata": {},
   "source": [
    "## Loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e6d27-7ae1-4f0a-bb2c-6c0f7b38d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd17122-1651-4899-a708-a80c8da8b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aacfb26-8d57-4b78-b0e8-5687f7465fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e15ee-966d-41df-8a70-6e7bf0683fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"tokenizers/bert-base-cased/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560a23e-3cef-4065-9dae-649fc44b6f7e",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7defec39-e17b-438d-82ca-ea5611fbc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f2254-5827-4b8f-9434-7794cf80c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6fbb8-7edf-40ee-8575-9cc4b8bd7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb4e6c-426d-4496-952f-edc0a1b44332",
   "metadata": {},
   "source": [
    "## From tokens to input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d85e1-8121-40db-beb8-1afa49563f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05155e4-4a79-4e52-bd64-f01312652e1f",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15834e-556f-4f24-90d7-98557f4c4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96834a-46ca-4ea3-8420-9d86d38469c6",
   "metadata": {},
   "source": [
    "# Handling multiple sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0e5b6-2472-47d2-8a32-a1a0b3477c7e",
   "metadata": {},
   "source": [
    "## Models expect a batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584e839-0580-4849-8406-6d06d680e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0b5e9-29e2-42d6-9353-b555f617a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884b316-2f80-4788-96de-e7cd58dd0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27afcb-3768-4f74-960b-23eca2cc611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# this line will faile\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05642ed-fe3a-466c-a836-cd81a2d67754",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0e1c0-d559-4fc0-920a-3d2f1b654c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fd8e4-f89f-4050-8bd9-e7e27da6b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524821f3-466d-4ca6-961e-b039acda0232",
   "metadata": {},
   "source": [
    "## Padding the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d8fa2-5f20-4168-813b-dc7f905b7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501672a3-be75-42c6-bf9f-c7d099c14006",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd1e94-0db6-4b40-8d10-d85c5efd36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be841d85-e4d8-4a2f-82f2-e88ce8e5a448",
   "metadata": {},
   "source": [
    "## Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ca542-86af-4010-8a85-9d8064e0cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035370a-027f-447f-a759-b58146a2cb83",
   "metadata": {},
   "source": [
    "## Longer sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05befbf9-b935-4a45-b654-cf9f4e488541",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd1386-c3de-4a78-b09d-30d4e03f61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85f36b-d5fb-43cc-812d-51190549f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24df467-bf7c-49cd-85c8-0578eba081ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694dbfb-60c3-45bf-95f5-2389791b1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d85259-25a7-42c4-a353-6d73de584b7f",
   "metadata": {},
   "source": [
    "## Special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcf4e6-fbd9-43f6-a4ed-f39b077b3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ceac6-abfc-4d18-bb2e-febf3b5f0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3b29d-85a2-4696-9851-918dbe403d85",
   "metadata": {},
   "source": [
    "## Wrapping up: From tokenizer to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63630e64-530c-407f-8ffb-994d8d4e7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2a6ac-9bd0-4a8b-991d-316b4ff575d3",
   "metadata": {},
   "source": [
    "# Optimzed Inference Deployment\n",
    "\n",
    "https://huggingface.co/learn/llm-course/en/chapter2/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b8260-b84b-44f8-91a1-f28a3cb1452f",
   "metadata": {},
   "source": [
    "## Installation and Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8cb61-5431-4577-a951-93a947770a3f",
   "metadata": {},
   "source": [
    "```\n",
    "docker run --gpus all \\\n",
    "    --shm-size 1g \\\n",
    "    -p 8080:80 \\\n",
    "    -v ~/.cache/huggingface:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f685755-c334-4fe9-98f1-002a65236ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
